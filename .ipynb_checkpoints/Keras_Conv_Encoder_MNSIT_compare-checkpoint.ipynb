{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.callbacks import TensorBoard\n",
    "# from tensorflow.keras.callbacks import TensorBoard\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_img = Input(shape=(28, 28, 1))    # adapt this if using 'channels_first' image data format\n",
    "\n",
    "x = Conv2D(16, (3, 3), activation='relu', padding='same')(input_img)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = MaxPooling2D((2, 2), padding='same')(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "encoded = MaxPooling2D((2, 2), padding='same')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# at this point the representation is (4, 4, 8), i.e. 128-dimensional\n",
    "\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(encoded)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(8, (3, 3), activation='relu', padding='same')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "x = Conv2D(16, (3, 3), activation='relu')(x)\n",
    "x = UpSampling2D((2, 2))(x)\n",
    "decoded = Conv2D(1, (3, 3), activation='sigmoid', padding='same')(x)\n",
    "\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To train it, use the original MNIST digits with shape (samples, 3, 28, 28),\n",
    "# and just normalize pixel values between 0 and 1\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = np.reshape(x_train, (len(x_train), 28, 28, 1))    # adapt this if using 'channels_first' image data format\n",
    "x_test = np.reshape(x_test, (len(x_test), 28, 28, 1))       # adapt this if using 'channels_first' image data format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      " - 6s - loss: 0.2224 - val_loss: 0.1656\n",
      "Epoch 2/50\n",
      " - 6s - loss: 0.1543 - val_loss: 0.1424\n",
      "Epoch 3/50\n",
      " - 6s - loss: 0.1413 - val_loss: 0.1375\n",
      "Epoch 4/50\n",
      " - 6s - loss: 0.1346 - val_loss: 0.1294\n",
      "Epoch 5/50\n",
      " - 6s - loss: 0.1300 - val_loss: 0.1286\n",
      "Epoch 6/50\n",
      " - 6s - loss: 0.1265 - val_loss: 0.1252\n",
      "Epoch 7/50\n",
      " - 6s - loss: 0.1241 - val_loss: 0.1205\n",
      "Epoch 8/50\n",
      " - 6s - loss: 0.1219 - val_loss: 0.1199\n",
      "Epoch 9/50\n",
      " - 6s - loss: 0.1200 - val_loss: 0.1190\n",
      "Epoch 10/50\n",
      " - 6s - loss: 0.1183 - val_loss: 0.1169\n",
      "Epoch 11/50\n",
      " - 6s - loss: 0.1168 - val_loss: 0.1157\n",
      "Epoch 12/50\n",
      " - 6s - loss: 0.1152 - val_loss: 0.1109\n",
      "Epoch 13/50\n",
      " - 6s - loss: 0.1145 - val_loss: 0.1131\n",
      "Epoch 14/50\n",
      " - 7s - loss: 0.1134 - val_loss: 0.1155\n",
      "Epoch 15/50\n",
      " - 6s - loss: 0.1126 - val_loss: 0.1114\n",
      "Epoch 16/50\n",
      " - 7s - loss: 0.1118 - val_loss: 0.1092\n",
      "Epoch 17/50\n",
      " - 7s - loss: 0.1110 - val_loss: 0.1094\n",
      "Epoch 18/50\n",
      " - 6s - loss: 0.1107 - val_loss: 0.1108\n",
      "Epoch 19/50\n",
      " - 6s - loss: 0.1101 - val_loss: 0.1090\n",
      "Epoch 20/50\n",
      " - 7s - loss: 0.1096 - val_loss: 0.1082\n",
      "Epoch 21/50\n",
      " - 6s - loss: 0.1089 - val_loss: 0.1065\n",
      "Epoch 22/50\n",
      " - 6s - loss: 0.1084 - val_loss: 0.1057\n",
      "Epoch 23/50\n",
      " - 7s - loss: 0.1079 - val_loss: 0.1060\n",
      "Epoch 24/50\n",
      " - 6s - loss: 0.1077 - val_loss: 0.1053\n",
      "Epoch 25/50\n",
      " - 6s - loss: 0.1074 - val_loss: 0.1051\n",
      "Epoch 26/50\n",
      " - 6s - loss: 0.1068 - val_loss: 0.1081\n",
      "Epoch 27/50\n",
      " - 6s - loss: 0.1065 - val_loss: 0.1061\n",
      "Epoch 28/50\n",
      " - 6s - loss: 0.1064 - val_loss: 0.1014\n",
      "Epoch 29/50\n",
      " - 6s - loss: 0.1057 - val_loss: 0.1063\n",
      "Epoch 30/50\n",
      " - 6s - loss: 0.1057 - val_loss: 0.1042\n",
      "Epoch 31/50\n",
      " - 6s - loss: 0.1053 - val_loss: 0.1020\n",
      "Epoch 32/50\n",
      " - 6s - loss: 0.1050 - val_loss: 0.1043\n",
      "Epoch 33/50\n",
      " - 6s - loss: 0.1046 - val_loss: 0.1056\n",
      "Epoch 34/50\n",
      " - 6s - loss: 0.1044 - val_loss: 0.1077\n",
      "Epoch 35/50\n",
      " - 6s - loss: 0.1041 - val_loss: 0.1040\n",
      "Epoch 36/50\n",
      " - 6s - loss: 0.1039 - val_loss: 0.1026\n",
      "Epoch 37/50\n",
      " - 6s - loss: 0.1038 - val_loss: 0.1063\n",
      "Epoch 38/50\n",
      " - 6s - loss: 0.1034 - val_loss: 0.1049\n",
      "Epoch 39/50\n",
      " - 6s - loss: 0.1031 - val_loss: 0.1001\n",
      "Epoch 40/50\n",
      " - 6s - loss: 0.1028 - val_loss: 0.1039\n",
      "Epoch 41/50\n",
      " - 6s - loss: 0.1029 - val_loss: 0.1048\n",
      "Epoch 42/50\n",
      " - 6s - loss: 0.1028 - val_loss: 0.1016\n",
      "Epoch 43/50\n",
      " - 6s - loss: 0.1025 - val_loss: 0.0993\n",
      "Epoch 44/50\n",
      " - 6s - loss: 0.1021 - val_loss: 0.1003\n",
      "Epoch 45/50\n",
      " - 6s - loss: 0.1019 - val_loss: 0.1007\n",
      "Epoch 46/50\n",
      " - 6s - loss: 0.1020 - val_loss: 0.1005\n",
      "Epoch 47/50\n",
      " - 6s - loss: 0.1017 - val_loss: 0.1017\n",
      "Epoch 48/50\n",
      " - 6s - loss: 0.1016 - val_loss: 0.1006\n",
      "Epoch 49/50\n",
      " - 6s - loss: 0.1014 - val_loss: 0.1011\n",
      "Epoch 50/50\n",
      " - 6s - loss: 0.1015 - val_loss: 0.0988\n",
      " .. MODEL FITTING DONE ..\n"
     ]
    }
   ],
   "source": [
    "# open a terminal and start TensorBoard to read logs in the autoencoder subdirectory\n",
    "# tensorboard --logdir=autoencoder\n",
    "# tbCallBack = TensorBoard(log_dir='./Graph', histogram_freq=0, write_graph=True, write_images=True)\n",
    "autoencoder.fit(x_train, x_train, epochs=50, batch_size=128, shuffle=True, validation_data=(x_test, x_test), verbose=2)\n",
    "print(\" .. MODEL FITTING DONE ..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at the 128-dimensional encoded representation\n",
    "# these representations are 8x4x4, so we reshape them to 4x32 in order to be able to display them as grayscale images\n",
    "\n",
    "encoder = Model(input_img, encoded)\n",
    "encoded_test_imgs = encoder.predict(x_test)\n",
    "\n",
    "# save latent space features 128-d vector\n",
    "pickle.dump(encoded_test_imgs, open('conv_autoe_features.pickle', 'wb'))\n",
    "\n",
    "# n = 10\n",
    "# plt.figure(figsize=(10, 4), dpi=100)\n",
    "# for i in range(n):\n",
    "#     ax = plt.subplot(1, n, i + 1)\n",
    "#     plt.imshow(encoded_imgs[i].reshape(4, 4 * 8).T)\n",
    "#     plt.gray()\n",
    "#     ax.set_axis_off()\n",
    "\n",
    "# plt.show()\n",
    "\n",
    "# K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "index = 0\n",
    "target_img = x_test[index]\n",
    "target_label = y_test[index]\n",
    "# encoded_target_img = encoder.predict(target_img)\n",
    "\n",
    "# plt.figure(figsize=(10, 4), dpi=100)\n",
    "plt.figure()\n",
    "# display original\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "plt.imshow(target_img.reshape(28, 28))\n",
    "plt.gray()\n",
    "ax.set_xlabel(target_label, fontsize=10)\n",
    "# ax.set_axis_off()\n",
    "\n",
    "# display Encoded Image\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "plt.imshow(encoded_test_imgs[index].reshape(4, 4 * 8).T)\n",
    "plt.gray()\n",
    "# ax.set_axis_off()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import hamming, euclidean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 980), (1, 1135), (2, 1032), (3, 1010), (4, 982), (5, 892), (6, 958), (7, 1028), (8, 974), (9, 1009)]\n",
      "10000\n"
     ]
    }
   ],
   "source": [
    "total = []\n",
    "sum = 0\n",
    "for j in range(10):\n",
    " count = 0\n",
    " for i in y_test:\n",
    "  if i == j:\n",
    "    count += 1\n",
    " sum += count\n",
    " total.append((j,count))\n",
    "\n",
    "print(total)\n",
    "print(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.2\n",
    "output = []   # Holds a tuple : (distance, image)\n",
    "score = 0\n",
    "\n",
    "for i in range(10000):\n",
    "    distance = euclidean(encoded_test_imgs[index].reshape(-1), encoded_test_imgs[i].reshape(-1)) # Euclidean could be replaced by Hamming , Euclidean , cityblock (Manhattan) , chebyshev , cosine \n",
    "#     print(distance)\n",
    "    if distance < threshold:               # Every distance metric will have different thresgold value\n",
    "#         if distance != 0: \n",
    "        label = y_test[i]\n",
    "        result = (distance, x_test[i], label)\n",
    "        if label == target_label:\n",
    "            score += 1\n",
    "        output.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Image label: 7\n",
      "Total number of samples: 1028\n",
      "Threshold value: 0.2\n",
      "\n",
      "Number of images retrieved: 1\n",
      "True positives: 1\n",
      "False positives: 0\n",
      "Accuracy of retrieved samples:100.00%\n",
      "Percent of actual samples retrieved:0.10 %\n"
     ]
    }
   ],
   "source": [
    "retrieved = len(output)\n",
    "total_samples = total[target_label][1]\n",
    "print(\"Target Image label:\",target_label)\n",
    "print(\"Total number of samples:\", total_samples)\n",
    "print(\"Threshold value:\",threshold)\n",
    "\n",
    "print(\"\\nNumber of images retrieved:\",retrieved)\n",
    "print(\"True positives:\",score)\n",
    "print(\"False positives:\",retrieved - score)\n",
    "print(\"Accuracy of retrieved samples:{:.2f}%\".format((score/retrieved)*100))\n",
    "print(\"Percent of actual samples retrieved:{:.2f} %\".format((retrieved/total_samples)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
