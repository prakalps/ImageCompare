{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distance measurement and vector comparison #\n",
    "\n",
    "Distance measures play an important role in machine learning.\n",
    "\n",
    "They provide the foundation for many popular and effective machine learning algorithms like k-nearest neighbors for supervised learning and k-means clustering for unsupervised learning.\n",
    "\n",
    "Different distance measures must be chosen and used depending on the types of the data. As such, it is important to know how to implement and calculate a range of different popular distance measures and the intuitions for the resulting scores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/long road.jpg\" alt=\"Drawing\" align=\"center\" style=\"width: 600px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clustering, or cluster analysis, is used for analyzing data which does not include pre-labeled classes. Data instances are grouped together using the concept of maximizing intraclass similarity and minimizing the similarity between differing classes. This translates to the clustering algorithm identifying and grouping instances which are very similar, as opposed to ungrouped instances which are much less-similar to one another. As clustering does not require the pre-labeling of classes, it is a form of unsupervised learning.\n",
    "\n",
    "At the core of cluster analysis is the concept of measuring distances between a variety of different data point dimensions. For example, when considering k-means clustering, there is a need to measure a) distances between individual data point dimensions and the corresponding cluster centroid dimensions of all clusters, and b) distances between cluster centroid dimensions and all resulting cluster member data point dimensions.\n",
    "\n",
    "While k-means, the simplest and most prominent clustering algorithm, generally uses Euclidean distance as its similarity distance measurement, contriving innovative or variant clustering algorithms which, among other alterations, utilize different distance measurements is not a stretch. Translation: using different techniques for cluster-related distance measurement is quite easily doable. However, the reasons for actually doing so would require great knowledge of both domain and data.\n",
    "\n",
    "We will leave the \"why\" of pursuing particular distance measurements out of this discussion, and instead quickly introduce five perfectly valid ways of measuring distances between data points.\n",
    "\n",
    "For more on the distance measurements that are available in the SciPy spatial.distance module, https://docs.scipy.org/doc/scipy/reference/spatial.distance.html#module-scipy.spatial.distance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/distances-kmeans-diagram.jpg\" alt=\"Drawing\" align=\"center\" style=\"width: 530px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <center>Fig. 1: A simple overview of the k-means clustering algorithm process.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A short list of some of the more popular machine learning algorithms that use distance measures at their core is as follows:\n",
    "\n",
    "- K-Nearest Neighbors\n",
    "- Learning Vector Quantization (LVQ)\n",
    "- Self-Organizing Map (SOM)\n",
    "- K-Means Clustering\n",
    "\n",
    "There are many kernel-based methods may also be considered distance-based algorithms. Perhaps the most widely known kernel method is the support vector machine (SVM) algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When calculating the distance between two examples or rows of data, it is possible that different data types are used for different columns of the examples. An example might have real values, boolean values, categorical values, and ordinal values. Different distance measures may be required for each that are summed together into a single distance score.\n",
    "\n",
    "Numerical values may have different scales. This can greatly impact the calculation of distance measure and it is often a good practice to normalize or standardize numerical values prior to calculating the distance measure.\n",
    "\n",
    "As we can see, distance measures play an important role in machine learning. Perhaps four of the most commonly used distance measures in machine learning are as follows:\n",
    "\n",
    "- Hamming Distance\n",
    "- Euclidean Distance\n",
    "- Manhattan Distance\n",
    "- Minkowski Distance\n",
    "- Chebyshev distance\n",
    "- Canberra distance\n",
    "- Cosine distance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamming Distance \n",
    "\n",
    "Hamming distance calculates the distance between two binary vectors, also referred to as binary strings or bitstrings for short.\n",
    "You are most likely going to encounter bitstrings when you one-hot encode categorical columns of data.\n",
    "\n",
    "For example, if a column had the categories ‘red,’ ‘green,’ and ‘blue,’ you might one hot encode each example as a bitstring with one bit for each column.\n",
    "\n",
    "- red = [1, 0, 0]\n",
    "- green = [0, 1, 0]\n",
    "- blue = [0, 0, 1]\n",
    "\n",
    "The distance between red and green could be calculated as the sum or the average number of bit differences between the two bitstrings. This is the Hamming distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# calculate hamming distance\n",
    "def hamming_distance(a, b):\n",
    "\treturn sum(abs(e1 - e2) for e1, e2 in zip(a, b)) / len(a)\n",
    " \n",
    "# define data\n",
    "row1 = [0, 0, 0, 0, 0, 1]\n",
    "row2 = [0, 0, 0, 0, 1, 0]\n",
    "# calculate distance\n",
    "dist = hamming_distance(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the same calculation using the hamming() function from SciPy. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3333333333333333\n"
     ]
    }
   ],
   "source": [
    "# calculating hamming distance between bit strings\n",
    "from scipy.spatial.distance import hamming\n",
    "# define data\n",
    "row1 = [0, 0, 0, 0, 0, 1]\n",
    "row2 = [0, 0, 0, 0, 1, 0]\n",
    "# calculate distance\n",
    "dist = hamming(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean Distance\n",
    "\n",
    "Euclidean distance calculates the distance between two real-valued vectors.\n",
    "\n",
    "You are most likely to use Euclidean distance when calculating the distance between two rows of data that have numerical values, such a floating point or integer values.\n",
    "\n",
    "If columns have values with differing scales, it is common to normalize or standardize the numerical values across all columns prior to calculating the Euclidean distance. Otherwise, columns that have large values will dominate the distance measure.\n",
    "\n",
    "Euclidean distance is calculated as the square root of the sum of the squared differences between the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/gif.gif\" alt=\"Drawing\" style=\"width: 200px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the distance calculation is to be performed thousands or millions of times, it is common to remove the square root operation in an effort to speed up the calculation. The resulting scores will have the same relative proportions after this modification and can still be used effectively within a machine learning algorithm for finding the most similar examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.082762530298219\n"
     ]
    }
   ],
   "source": [
    "# calculating euclidean distance between vectors\n",
    "from math import sqrt\n",
    " \n",
    "# calculate euclidean distance\n",
    "def euclidean_distance(a, b):\n",
    "\treturn sqrt(sum((e1-e2)**2 for e1, e2 in zip(a,b)))\n",
    " \n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "# calculate distance\n",
    "dist = euclidean_distance(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the same calculation using the euclidean() function from SciPy. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.082762530298219\n"
     ]
    }
   ],
   "source": [
    "# calculating euclidean distance between vectors\n",
    "from scipy.spatial.distance import euclidean\n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "# calculate distance\n",
    "dist = euclidean(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manhattan Distance (Taxicab or City Block Distance)\n",
    "\n",
    "The Manhattan distance, also called the Taxicab distance or the City Block distance, calculates the distance between two real-valued vectors.\n",
    "\n",
    "It is perhaps more useful to vectors that describe objects on a uniform grid, like a chessboard or city blocks. The taxicab name for the measure refers to the intuition for what the measure calculates: the shortest path that a taxicab would take between city blocks (coordinates on the grid)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Manhattan_distance.png\" alt=\"Drawing\" style=\"width: 300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### It might make sense to calculate Manhattan distance instead of Euclidean distance for two vectors in an integer feature space.\n",
    "\n",
    "Manhattan distance is calculated as the sum of the absolute differences between the two vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Manhattan_distance_formula.svg\" alt=\"Drawing\" style=\"width: 300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# calculating manhattan distance between vectors\n",
    "from math import sqrt\n",
    " \n",
    "# calculate manhattan distance\n",
    "def manhattan_distance(a, b):\n",
    "\treturn sum(abs(e1-e2) for e1, e2 in zip(a,b))\n",
    " \n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "# calculate distance\n",
    "dist = manhattan_distance(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the same calculation using the cityblock() function from SciPy. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    }
   ],
   "source": [
    "# calculating manhattan distance between vectors\n",
    "from scipy.spatial.distance import cityblock\n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "# calculate distance\n",
    "dist = cityblock(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski Distance\n",
    "\n",
    "Minkowski distance calculates the distance between two real-valued vectors.\n",
    "\n",
    "It is a generalization of the Euclidean and Manhattan distance measures and adds a parameter, called the “order” or “p“, that allows different distance measures to be calculated.\n",
    "\n",
    "The Minkowski distance measure is calculated as follows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- EuclideanDistance = (sum for i to N (abs(v1[i] – v2[i]))^p)^(1/p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where “p” is the order parameter.\n",
    "\n",
    "When p is set to 1, the calculation is the same as the Manhattan distance. When p is set to 2, it is the same as the Euclidean distance.\n",
    "\n",
    "- p=1: Manhattan distance.\n",
    "- p=2: Euclidean distance.\n",
    "\n",
    "Intermediate values provide a controlled balance between the two measures.\n",
    "\n",
    "It is common to use Minkowski distance when implementing a machine learning algorithm that uses distance measures as it gives control over the type of distance measure used for real-valued vectors via a hyperparameter “p” that can be tuned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "6.082762530298219\n"
     ]
    }
   ],
   "source": [
    "# calculating minkowski distance between vectors\n",
    "from math import sqrt\n",
    " \n",
    "# calculate minkowski distance\n",
    "def minkowski_distance(a, b, p):\n",
    "\treturn sum(abs(e1-e2)**p for e1, e2 in zip(a,b))**(1/p)\n",
    " \n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "# calculate distance (p=1)\n",
    "dist = minkowski_distance(row1, row2, 1)\n",
    "print(dist)\n",
    "# calculate distance (p=2)\n",
    "dist = minkowski_distance(row1, row2, 2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform the same calculation using the minkowski_distance() function from SciPy. The complete example is listed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.0\n",
      "6.082762530298219\n"
     ]
    }
   ],
   "source": [
    "# calculating minkowski distance between vectors\n",
    "from scipy.spatial import minkowski_distance\n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "# calculate distance (p=1)\n",
    "dist = minkowski_distance(row1, row2, 1)\n",
    "print(dist)\n",
    "# calculate distance (p=2)\n",
    "dist = minkowski_distance(row1, row2, 2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Chebyshev distance\n",
    "\n",
    "Chebyshev -- also chessboard -- distance is best defined as a distance metric \"where the distance between two vectors is the greatest of their differences along any coordinate dimension.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/chebyshev-distance-chessboard.jpg\" alt=\"Drawing\" style=\"width: 300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "# calculating minkowski distance between vectors\n",
    "import scipy.spatial.distance as dist\n",
    "import numpy as np\n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "dist = dist.chebyshev(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Canberra distance\n",
    "\n",
    "Canberra distance is a weighted version of Manhattan distance, which \"has been used as a metric for comparing ranked lists and for intrusion detection in computer security.\"\n",
    "\n",
    "Canberra distance can be represented as"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Canberra_distance_formula.svg\" alt=\"Drawing\" style=\"width: 200px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where p and q are vectors and"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"Images/Canberra.svg\" alt=\"Drawing\" style=\"width: 300px;\" align=\"center\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cosine distance\n",
    "\n",
    "Cosine similarity is defined as:\n",
    "\n",
    "- A measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them. The cosine of 0° is 1, and it is less than 1 for any other angle. It is thus a judgment of orientation and not magnitude: two vectors with the same orientation have a cosine similarity of 1, two vectors at 90° have a similarity of 0, and two vectors diametrically opposed have a similarity of -1, independent of their magnitude. Cosine similarity is particularly used in positive space, where the outcome is neatly bounded in [0,1].\n",
    "\n",
    "Cosine similarity is often used in clustering to assess cohesion, as opposed to determining cluster membership."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006746536411526116\n"
     ]
    }
   ],
   "source": [
    "# calculating minkowski distance between vectors\n",
    "import scipy.spatial.distance as dist\n",
    "import numpy as np\n",
    "# define data\n",
    "row1 = [10, 20, 15, 10, 5]\n",
    "row2 = [12, 24, 18, 8, 7]\n",
    "dist = dist.cosine(row1, row2)\n",
    "print(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison of vairous distance measuring methods using SCIPY lib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2 10-dimensional vectors\n",
      "------------------------\n",
      "[2.09523466 8.8584794  7.10525173 4.21019878 3.92549156 9.95041411\n",
      " 1.90674413 1.35585059 1.8135173  1.84081422]\n",
      "[6.65407267 2.43015245 4.34135714 5.83665812 9.49577103 2.40288214\n",
      " 4.28899003 3.08669304 0.09287104 3.68865699]\n",
      "\n",
      "Distance measurements with 10-dimensional vectors\n",
      "-------------------------------------------------\n",
      "\n",
      "Euclidean distance is 13.245006013035148\n",
      "Manhattan distance is 36.17690769068824\n",
      "Chebyshev distance is 7.547531964078414\n",
      "Canberra distance is 4.530710085972678\n",
      "Cosine distance is 0.33682432179226585\n",
      "\n",
      "Distance measurements with 100-dimensional vectors\n",
      "--------------------------------------------------\n",
      "\n",
      "Euclidean distance is 40.95743087035269\n",
      "Manhattan distance is 342.5038619689756\n",
      "Chebyshev distance is 9.716099368956781\n",
      "Canberra distance is 40.28931934371655\n",
      "Cosine distance is 0.26487654853187403\n"
     ]
    }
   ],
   "source": [
    "import scipy.spatial.distance as dist\n",
    "import numpy as np\n",
    "\n",
    "# Prepare 2 vectors (data points) of 10 dimensions\n",
    "A = np.random.uniform(0, 10, 10)\n",
    "B = np.random.uniform(0, 10, 10)\n",
    "\n",
    "print('\\n2 10-dimensional vectors')\n",
    "print('------------------------')\n",
    "print(A)\n",
    "print(B)\n",
    "\n",
    "# Perform distance measurements \n",
    "print('\\nDistance measurements with 10-dimensional vectors')\n",
    "print('-------------------------------------------------')\n",
    "print('\\nEuclidean distance is', dist.euclidean(A, B))\n",
    "print('Manhattan distance is', dist.cityblock(A, B))\n",
    "print('Chebyshev distance is', dist.chebyshev(A, B))\n",
    "print('Canberra distance is', dist.canberra(A, B))\n",
    "print('Cosine distance is', dist.cosine(A, B))\n",
    "\n",
    "# Prepare 2 vectors of 100 dimensions\n",
    "AA = np.random.uniform(0, 10, 100)\n",
    "BB = np.random.uniform(0, 10, 100)\n",
    "\n",
    "# Perform distance measurements\n",
    "print('\\nDistance measurements with 100-dimensional vectors')\n",
    "print('--------------------------------------------------')\n",
    "print('\\nEuclidean distance is', dist.euclidean(AA, BB))\n",
    "print('Manhattan distance is', dist.cityblock(AA, BB))\n",
    "print('Chebyshev distance is', dist.chebyshev(AA, BB))\n",
    "print('Canberra distance is', dist.canberra(AA, BB))\n",
    "print('Cosine distance is', dist.cosine(AA, BB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
